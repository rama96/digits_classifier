{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-02T18:22:52.478199Z","iopub.execute_input":"2022-07-02T18:22:52.478551Z","iopub.status.idle":"2022-07-02T18:22:52.485502Z","shell.execute_reply.started":"2022-07-02T18:22:52.478518Z","shell.execute_reply":"2022-07-02T18:22:52.484548Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\ntrain_df.shape , test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:22:52.736748Z","iopub.execute_input":"2022-07-02T18:22:52.737560Z","iopub.status.idle":"2022-07-02T18:22:57.694877Z","shell.execute_reply.started":"2022-07-02T18:22:52.737524Z","shell.execute_reply":"2022-07-02T18:22:57.693892Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:22:57.696581Z","iopub.execute_input":"2022-07-02T18:22:57.697252Z","iopub.status.idle":"2022-07-02T18:22:57.705024Z","shell.execute_reply.started":"2022-07-02T18:22:57.697214Z","shell.execute_reply":"2022-07-02T18:22:57.703527Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor , Lambda\n\nclass CustomMNISTDataset(Dataset):\n    def __init__(self, csv_name, img_dir, transform=None, target_transform=None , label_name = \"label\"):\n        \n        self.img_filename = csv_name\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.label_name = label_name\n        \n        img_path = os.path.join(self.img_dir, self.img_filename)\n        self.img_df = pd.read_csv(img_path)\n\n    def __len__(self):\n        return len(self.img_df)\n\n    def __getitem__(self, idx):\n        \n        # Extracting all the other columns except label_name\n        img_cols = [ i for i in self.img_df.columns if i not in self.label_name]\n        \n        image = self.img_df.iloc[[idx]][img_cols].values\n        label = int(self.img_df.iloc[[idx]][self.label_name].values)\n        \n        # Reshaping the array from 1*784 to 28*28\n        \n        # image = image.reshape(28,28)\n        # image = image.astype(float)\n        \n        # Scaling the image so that the values only range between 0 and 1\n        # image = image/255.0\n        \n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        \n        image = image.to(torch.float)\n        label = label.to(torch.float)\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:22:59.812815Z","iopub.execute_input":"2022-07-02T18:22:59.813166Z","iopub.status.idle":"2022-07-02T18:23:01.685008Z","shell.execute_reply.started":"2022-07-02T18:22:59.813136Z","shell.execute_reply":"2022-07-02T18:23:01.684062Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntrain_df['label'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:23:03.539735Z","iopub.execute_input":"2022-07-02T18:23:03.541012Z","iopub.status.idle":"2022-07-02T18:23:05.768517Z","shell.execute_reply.started":"2022-07-02T18:23:03.540968Z","shell.execute_reply":"2022-07-02T18:23:05.767524Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Illustration of creating a validation set \n\nfrom sklearn.model_selection import train_test_split\nindices = list(range(len(train_df)))\n\ntrain_indices , test_indices = train_test_split(indices, test_size=0.1, stratify=train_df['label'])\n# train_indices , test_indices = train_test_split(indices, test_size=0.1)\n\nlen(train_indices) , len(test_indices) , len(train_df)\n\ntrain_subset = train_df.loc[train_indices]\nval_subset = train_df.loc[test_indices]\n\nprint( train_subset['label'].value_counts().sort_index() / train_subset['label'].value_counts().sort_index().sum() )\nprint( val_subset['label'].value_counts().sort_index() / val_subset['label'].value_counts().sort_index().sum() )","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:23:07.016085Z","iopub.execute_input":"2022-07-02T18:23:07.017091Z","iopub.status.idle":"2022-07-02T18:23:08.043822Z","shell.execute_reply.started":"2022-07-02T18:23:07.017043Z","shell.execute_reply":"2022-07-02T18:23:08.042680Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# Crerating a temp dataset\ntrain_csv_name = \"train.csv\"\ntest_csv_name = \"test.csv\"\nimg_dir = \"/kaggle/input/digit-recognizer/\"\n\n# Converting X variables to Tensors\ntransforms = ToTensor()\n\n# Converting y-labels to one hot encoding\ntarget_transform = Lambda(lambda y: torch.zeros(\n    len(train_df['label'].unique()), dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n\nlabel_name = \"label\"\n\ntrain_dataset = CustomMNISTDataset(csv_name = train_csv_name , img_dir = img_dir , transform = transforms , target_transform = target_transform , label_name = label_name)\n# test_dataset = CustomMNISTDataset(csv_name = test_csv_name , img_dir = img_dir , transform = transforms , target_transform = target_transform , label_name = label_name)\nx0 , y0 = train_dataset[0]\nprint(x0.shape , y0.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:23:10.745597Z","iopub.execute_input":"2022-07-02T18:23:10.746517Z","iopub.status.idle":"2022-07-02T18:23:13.431310Z","shell.execute_reply.started":"2022-07-02T18:23:10.746479Z","shell.execute_reply":"2022-07-02T18:23:13.430293Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Checking if the GPU is being used properly . \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\ntorch.cuda.is_available()\nx0 = x0.to(device)\nprint(\"x0\" , x0.is_cuda)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:23:15.497838Z","iopub.execute_input":"2022-07-02T18:23:15.498186Z","iopub.status.idle":"2022-07-02T18:23:18.225212Z","shell.execute_reply.started":"2022-07-02T18:23:15.498158Z","shell.execute_reply":"2022-07-02T18:23:18.224246Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"torch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:23:20.903649Z","iopub.execute_input":"2022-07-02T18:23:20.903996Z","iopub.status.idle":"2022-07-02T18:23:20.910963Z","shell.execute_reply.started":"2022-07-02T18:23:20.903969Z","shell.execute_reply":"2022-07-02T18:23:20.909869Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import SubsetRandomSampler\nfrom sklearn.model_selection import train_test_split\nindices = list(range(len(train_df)))\ntrain_indices , test_indices = train_test_split(indices, test_size=0.1, stratify=train_df['label'])\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset , batch_size=64, sampler=train_sampler, num_workers=16)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=test_sampler, num_workers=16)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:24:40.595248Z","iopub.execute_input":"2022-07-02T18:24:40.595635Z","iopub.status.idle":"2022-07-02T18:24:40.631099Z","shell.execute_reply.started":"2022-07-02T18:24:40.595580Z","shell.execute_reply":"2022-07-02T18:24:40.630015Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x0 , y0 = next(iter(train_loader))\nx0.shape , y0.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:24:42.700193Z","iopub.execute_input":"2022-07-02T18:24:42.700546Z","iopub.status.idle":"2022-07-02T18:24:44.828049Z","shell.execute_reply.started":"2022-07-02T18:24:42.700517Z","shell.execute_reply":"2022-07-02T18:24:44.826962Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Creating data loaders using temp_dataset\nfrom torch.utils.data import DataLoader\n\n# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:44:34.343352Z","iopub.execute_input":"2022-07-02T12:44:34.343681Z","iopub.status.idle":"2022-07-02T12:44:34.348747Z","shell.execute_reply.started":"2022-07-02T12:44:34.343652Z","shell.execute_reply":"2022-07-02T12:44:34.347878Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\nclass MyOwnNeuralNetwork(nn.Module):\n    def __init__(self):\n        super(MyOwnNeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(784, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return torch.softmax(logits , dim = 1)\n    \nmodel = MyOwnNeuralNetwork().to(device)\nprint(model)\n\n# model = model.cuda()\n# torch.backends.cudnn.benchmark=True\n# torch.cuda.set_device(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:24:51.921622Z","iopub.execute_input":"2022-07-02T18:24:51.922533Z","iopub.status.idle":"2022-07-02T18:24:51.942440Z","shell.execute_reply.started":"2022-07-02T18:24:51.922494Z","shell.execute_reply":"2022-07-02T18:24:51.941377Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Defining optimizer and loss functions \n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:24:59.695364Z","iopub.execute_input":"2022-07-02T18:24:59.697932Z","iopub.status.idle":"2022-07-02T18:24:59.703942Z","shell.execute_reply.started":"2022-07-02T18:24:59.697898Z","shell.execute_reply":"2022-07-02T18:24:59.702883Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\ndef train(dataloader, model, loss_fn, optimizer):\n    \n    # Total size of dataset for reference\n    size = len(dataloader.dataset)\n    \n    # places your model into training mode\n    model.train()\n    \n    # Gives X , y for each batch\n    for batch, (X, y) in enumerate(dataloader):\n        \n        # Converting device to cuda\n        X, y = X.to(device), y.to(device)\n        model.to(device)\n        \n        # print(\"X : \" , X.is_cuda , \" , y : \", y.is_cuda )\n        \n        # dtype = torch.cuda.FloatTensor\n        # X = Variable(X).type(dtype)\n        # y = Variable(y).type(dtype)\n\n        # Compute prediction error / loss\n        # 1. Compute y_pred \n        # 2. Compute loss between y and y_pred using selectd loss function\n        \n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n\n        # Backpropagation on optimizing for loss\n        # 1. Sets gradients as 0 \n        # 2. Compute the gradients using back_prop\n        # 3. update the parameters using the gradients from step 2\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:29:52.014272Z","iopub.execute_input":"2022-07-02T18:29:52.014694Z","iopub.status.idle":"2022-07-02T18:29:52.025358Z","shell.execute_reply.started":"2022-07-02T18:29:52.014634Z","shell.execute_reply":"2022-07-02T18:29:52.022935Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"data = next(iter(test_dataloader))\ndata[0].shape , data[1].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:27:12.285936Z","iopub.execute_input":"2022-07-02T18:27:12.286367Z","iopub.status.idle":"2022-07-02T18:27:14.366706Z","shell.execute_reply.started":"2022-07-02T18:27:12.286320Z","shell.execute_reply":"2022-07-02T18:27:14.365625Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data[1]","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:58:54.887265Z","iopub.execute_input":"2022-07-02T12:58:54.887674Z","iopub.status.idle":"2022-07-02T12:58:54.901624Z","shell.execute_reply.started":"2022-07-02T12:58:54.887641Z","shell.execute_reply":"2022-07-02T12:58:54.900483Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_dataloader))\ndata[0].shape , data[1].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-02T12:44:34.392766Z","iopub.execute_input":"2022-07-02T12:44:34.393104Z","iopub.status.idle":"2022-07-02T12:44:34.529495Z","shell.execute_reply.started":"2022-07-02T12:44:34.393066Z","shell.execute_reply":"2022-07-02T12:44:34.528404Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    \n    # Total size of dataset for reference\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    \n    # Explanation given above\n    model.eval()\n\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        \n        # Gives X , y for each batch\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            model.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n    \n    ## Calculating loss based on loss function defined\n    test_loss /= num_batches\n    \n    ## Calculating Accuracy based on how many y match with y_pred\n    correct /= size\n    \n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:27:19.803284Z","iopub.execute_input":"2022-07-02T18:27:19.804004Z","iopub.status.idle":"2022-07-02T18:27:19.812441Z","shell.execute_reply.started":"2022-07-02T18:27:19.803965Z","shell.execute_reply":"2022-07-02T18:27:19.811196Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T18:29:54.888905Z","iopub.execute_input":"2022-07-02T18:29:54.889255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}